{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4181/6802: Tutorial 2\n",
    "## Support Vector Machines for Protein Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 1.3 (February 3, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the second of four tutorials in the course that will give you practical experience with biological data. The exercises can be completed during class or later this week. The due date for this assignment is **Monday, February 10, 2020**, before 11:59pm. Please email your tutorial to me at **rbeiko@dal.ca**. Questions that you need to answer are indicated in **boldface**. Each question is worth one point unless otherwise noted.\n",
    "\n",
    "The goal of this tutorial is to apply support vector machines to the characterization of proteins that contain membrane-spanning domains. These domains tend to be hydrophobic, enriched in certain types of amino acids (A, E, K, L, M), and deficient in others (D, G, P). We will try out the scikit-learn interface to the popular *libsvm* package to try and classify our sequences. Ultimately, our goal is to pick out true alpha-helix transmembrane sequences from a background of \"null\", or negative, data that is generated by shuffling the order of the amino acids in the alpha-helix transmembrane protein set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Pre-requisites\n",
    "\n",
    "If you were able to run Tutorial 1, you should have no issues with Tutorial 2. No additional software is required.\n",
    "\n",
    "In these tutorials, whenever you see a code block, you must enter it by clicking on it and execute it by hitting Shift+Enter. This will change the [ ] icon to the left to [\\*], indicating that the code is running. It will indicate a number once the code has finished executing. **Be sure to run every code block in order, as they depend on one another.**\n",
    "\n",
    "The next code block loads all of the required modules. If you have any installation issues, the following code block will generate an error. Make sure to run this first and e-mail rbeiko@dal.ca if you are having any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you can run this code block without errors, you have everything you need!\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-requisites\n",
    "\n",
    "The dataset we will use was downloaded from the PDBTM database, a subset of the Protein Data Bank (PDB) dedicated\n",
    "to transmembrane proteins. The page with URL http://pdbtm.enzim.hu/data/pdbtm_alpha.seq hosts a long list of\n",
    "protein sequences that are known to contain transmembrane domains; it is from this set that our positive cases were\n",
    "derived. Sequences with the same prefix are presumably homologous, so I chose to extract only the longest sequence\n",
    "from each homologous set. This accomplished three worthwhile goals:\n",
    "\n",
    "- (i) Reducing the size of the dataset\n",
    "- (ii) Eliminating short sequences with potentially uninformative compositional vectors\n",
    "- (iii) Reducing bias in the training set by including no more than one homolog from any given set\n",
    "\n",
    "Further to (ii) above, I also used a minimum length criterion to filter the sequences: to be included, a given sequence\n",
    "must be at least 100 amino acids long. These criteria reduced the size of our positive set from >2800 to 599 sequences.\n",
    "\n",
    "Let's take a peek at the data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2a06_P\n",
      "MTNIRKSHPLMKIVNNAFIDLPAPSNISSWWNFGSLLGICLILQILTGLFLAMHYTSDTTTAFSSVTHICRDVNYGWIIRYMHANGASMFFICLYMHVGRGLYYGSYTFLETWNIGVILLLTVMATAFMGYVLPWGQMSFWGATVITNLLSAIPYIGTNLVEWIWGGFSVDKATLTRFFAFHFILPFIIMAIAMVHLLFLHETGSNNPTGISSDVDKIPFHPYYTIKDILGALLLILALMLLVLFAPDLLGDPDNYTPANPLNTPPHIKPEWYFLFAYAILRSIPNKLGGVLALAFSILILALIPLLHTSKQRSMMFRPLSQCLFWALVADLLTLTWIGGQPVEHPYITIGQLASVLYFLLILVLMPTAGTIENKLLKW\n",
      ">2a0d_A\n",
      "MESPIQIFRGEPGPTCAPSACLPPNSSAWFPGWAEPDSNGSAGSEDAQLEPAHISPAIPVIITAVYSVVFVVGLVGNSLVMFVIIRYTKMKTATNIYIFNLALADALVTTTMPFQSTVYLMNSWPFGDVLCKIVISIDYYNMFTSIFTLTMMSVDRYIAVCHPVKALDFRTPLKAKIINICIWLLSSSVGISAIVLGGTKVREDVDVIECSLQFPDDDYSWWDLFMKICVFIFAFVIPVLIIIVCYTLMILRLKSVRLLSGSREKDRNLRRITRLVLVVVAVFVVCWTPIHIFILVEALGSTSHSTAALSSYYFCIALGYTNSSLNPILYAFLDENFKRCFRDFCFPLKMRMERQSTSRVRNTVQDPAYLRDIDGMNKPV\n"
     ]
    }
   ],
   "source": [
    "# Print out the first two lines of the file, just for a peek\n",
    "with open(\"tutorial2_appendix_data/AlphaTM.faa\", 'r') as alpha_tm:\n",
    "    print(alpha_tm.readline(), end='')\n",
    "    print(alpha_tm.readline(), end='')\n",
    "    print(alpha_tm.readline(), end='')\n",
    "    print(alpha_tm.readline(), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data file is in the text-based FASTA format file with extension *.faa* indicating that the sequences are amino acids, rather than nucleotides. There is one line for the label, and one line for the protein sequence. Our goal is to identify these postiviely as transmembrane proteins against a background of negative data. The sequence file is small (255KB) so let's read it into memory for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_sequences():\n",
    "    # Dictionary with protein ID as the key and the sequence as the value\n",
    "    positive_sequences = {}\n",
    "\n",
    "    with open(\"tutorial2_appendix_data/AlphaTM.faa\", 'r') as alpha_tm:\n",
    "        for label in alpha_tm:\n",
    "            # Cut out the FASTA label marker character > by excluding the first character\n",
    "            # Get rid of any newline characters with .strip()\n",
    "            label = label[1:].strip()\n",
    "            # Get the sequence on the next line, remove newline characters with .strip()\n",
    "            sequence = alpha_tm.readline().strip()\n",
    "            positive_sequences[label] = sequence\n",
    "    return positive_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative set for today's event will actually be generated directly from the positive set. This is in contrast to the last tutorial where we were using data from two organisms. Here we will generate a \"null distribution\" of sequences by taking each positive sequence, splitting it into fragments of length L, and randomly reassembling these fragments to generate a new sequence. So if L = 1, then we are shuffling the order of amino acids. But if L = 10, then long contiguous stretches of the protein are going to be preserved. We are breaking the sequences into adjacent words of length L, shuffling those words randomly, and then merging them together. This is a sampling without replacement strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def shuffle_sequences(positive_sequences, L = 1):\n",
    "    negative_sequences = {}\n",
    "    # Iterate through the key, value pairs of the positive_sequences dictionary\n",
    "    for label, positive_sequence in positive_sequences.items():\n",
    "        # Break the original into words, e.g., \"ABCDEFG\" becomes [\"AB\" \"CD\" \"EF\" \"G\"]\n",
    "        words = [ positive_sequence[i:i+L] for i in range(0, len(positive_sequence), L) ]\n",
    "        # Shuffles the words array in place, e.g., [\"CD\", \"G\", \"EF\", \"AB\"]\n",
    "        shuffle(words)\n",
    "        # Merge the shuffled words back together, e.g., \"CDGEFAB\"\n",
    "        negative_sequence = \"\".join(words)\n",
    "        # Store in the dictionary\n",
    "        negative_sequences[label] = negative_sequence\n",
    "        # This will raise an error if our negative sequences aren't the length we expect\n",
    "        assert(len(positive_sequence) == len(negative_sequence))\n",
    "    return negative_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting an Alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce the 20-letter amino acid alphabet by mapping the characters onto a smaller set. This can be done using empirical properties of the amino acids. Three mappings have been provided here. The first is simply the identity, which uses all of the amino acids. The second is the PAM 6-letter alphabet, which categorizes amino acids based on chemical properties granted by their side chains. The third is a 3-letter alphabet that maps amino acids as either positively charged, aromatic, or proline. Later in the tutorial, you will be asked to change the alphabet used and investigate the impact that a reduced alphabet has on the accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data set has 21 amino acid codes (20 actual amino acids + X for wildcard)\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWXY\"\n",
    "# Use the standard 20 amino acid alphabet plus X for wildcard\n",
    "amino_acid_alphabet = \"ACDEFGHIKLMNPQRSTVWXY\"\n",
    "\n",
    "# Map each of the 20 amino acids into 6 categories, encoded by A, B, C, D, E, and F (plus X) \n",
    "PAM6_mapping = {\"D\":\"A\", \"E\":\"A\", \"N\":\"A\", \"Q\":\"A\",  # Acidic and amide amino acids\n",
    "                \"W\":\"B\", \"F\":\"B\", \"Y\":\"B\", # Aromatic amino acids\n",
    "                \"H\":\"C\", \"K\":\"C\", \"R\":\"C\", # Basic amino acids\n",
    "                \"A\":\"D\", \"G\":\"D\", \"I\":\"D\", \"L\":\"D\", \"V\":\"D\", # Aliphatic amino acids\n",
    "                \"C\":\"E\", \"M\":\"E\", \"S\":\"E\", \"T\":\"E\", # Hydroxyl or sulphur amino acids\n",
    "                \"P\":\"F\", # Proline, all alone\n",
    "                \"X\":\"X\"} # Maintain wildcard amino acids as X\n",
    "# Put it into a string the same length as amino_acids, in corresponding order\n",
    "PAM6_alphabet = \"\".join( [ PAM6_mapping[ amino_acids[i] ] for i in range(0, len(amino_acids)) ] )\n",
    "\n",
    "# Map each of the 20 amino acids into 3 categories, encoded by A, B, and C (plus X)\n",
    "three_mapping = {\"M\":\"A\", \"A\":\"A\", \"L\":\"A\", \"E\":\"A\", \"K\":\"A\", # Positive\n",
    "                  \"C\":\"B\", \"D\":\"B\", \"F\":\"B\", \"G\":\"B\", \"H\":\"B\", \"I\":\"B\", \"N\":\"B\", # Aromatic\n",
    "                  \"Q\":\"B\", \"R\":\"B\", \"S\":\"B\", \"T\":\"B\", \"V\":\"B\", \"W\":\"B\", \"Y\":\"B\", # Aromatic continued\n",
    "                  \"P\":\"C\", # Proline\n",
    "                  \"X\":\"X\"} # Maintain wildcard amino acids as X\n",
    "# Put it into a string the same length as amino_acids, in corresponding order\n",
    "three_alphabet = \"\".join( [ three_mapping[ amino_acids[i] ] for i in range(0, len(amino_acids)) ] )\n",
    "\n",
    "# You can design your own alphabet mapping by creating a string that maps amino_acids onto the new characters\n",
    "# For example,\n",
    "awful_alphabet = \"AAAAAAAAAABBBBBBBBBBB\"\n",
    "# This alphabet will convert the amino acids ACDEFGHIKL to A and the amino acids MNPQRSTVWXY to B\n",
    "# Try your own, and select it in the next code block!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Protein Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As in Tutorial 1, we need to derive tractable features from our protein sequences. We will again count the amino acid words of length w. For example, the sequence \"MSIVLKVKVK\" with w = 3 would have the features and counts: MSI: 1, SIV: 1, IVL: 1, VLK: 1, LKV: 1, KVK: 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use islice to more efficiently count our words\n",
    "# This is in the standard Python library, so everyone should have this\n",
    "from itertools import islice\n",
    "\n",
    "# This function efficiently slides through a sequence, returning a list of words of length w\n",
    "def window(seq, w = 1):\n",
    "    \"Returns a sliding window (of width w) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[w-1]), (s1,s2,...,sw), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, w))\n",
    "    if len(result) == w:\n",
    "        yield \"\".join(result)\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield \"\".join(result)\n",
    "\n",
    "# This function iterates through the sequences, getting the word counts for each sequence\n",
    "def count_words(sequence_dictionary, w = 1):\n",
    "    # Store each protein's count dictionary in another dictionary\n",
    "    sample_dict = {}\n",
    "    for label, sequence in sequence_dictionary.items():\n",
    "        # Translate the sequence based on our dictionary ro reduce the alphabet\n",
    "        translated_sequence = sequence.translate(sequence.maketrans(amino_acids, my_alphabet))\n",
    "        # Use the above window function to get a list of words, e.g., [\"AA\", \"AC\", \"CA\", \"AC\", ...]\n",
    "        words = list(window(translated_sequence, w))\n",
    "        # Use np.unique to count the unique words and get two lists e.g.:\n",
    "        # unique = [\"AA\", \"AC, \"CA\", ...]\n",
    "        # counts = [1, 2, 1, ...]\n",
    "        unique, counts = np.unique(words, return_counts=True)\n",
    "        # Transform the two lists into a dictionary, e.g., {\"AA\": 1, \"AC\": 2, \"CA\": 1, ...}\n",
    "        feature_dict = dict((word, count) for word, count in zip(unique,counts))\n",
    "        # Put it as an entry in the sample_dict so we have one feature dict per protein sequence\n",
    "        sample_dict[label] = feature_dict\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a function that lets us invoke a Support Vector Machine-based classifier. This time, we will be using cross-validation to score the accuracy. If you'd like to learn more about the parameters C and gammma for the RBF kernel, see http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X, y, C = 1, gamma = 'auto', cv = 5):\n",
    "    classifier = SVC(kernel='rbf', C = C, gamma = gamma)\n",
    "    scores = cross_val_score(classifier, X, y, cv = cv)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below is the workhorse that calls all of our established code above. Make sure you've run all of the code blocks above this point. Once they have been run, only the code below needs to be re-run once the parameters have been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "########### BEGIN PARAMETERS THAT CAN BE MODIFIED ###########\n",
    "\n",
    "# Negative/null data creation parameter\n",
    "L = 1 # *** Shuffle parameter, defines the contiguous sequence lengths to shuffle for negative data ***\n",
    "\n",
    "# Feature creation parameter\n",
    "w = 1 # *** Word size parameter, defines the length of the words to use for the features ***\n",
    "\n",
    "# Alphabet mapping parameter\n",
    "my_alphabet = amino_acid_alphabet # *** Alphabet mapping, options: amino_acid_alphabet, PAM6_alphabet, three_alphabet\n",
    "\n",
    "# SVM parameters\n",
    "C = 1 # SVM penalty parameter\n",
    "gamma = 'auto' # SVM RBF kernel coefficient, 'auto' defaults to 1/n_features, but can be set to any float\n",
    "\n",
    "# Cross-validation parameters\n",
    "cv = 5 # Number of cross-validation folds to perform\n",
    "\n",
    "########### END PARAMETERS THAT CAN BE MODIFIED ###########\n",
    "\n",
    "# Get the positive sequences from the data file, using the function described above\n",
    "positive_sequences = get_positive_sequences()\n",
    "\n",
    "# Count the words in our positive_sequences {label: sequence} dictionary\n",
    "positive_sample_dict = count_words(positive_sequences, w = w)\n",
    "\n",
    "# Convert to Pandas DataFrame for easy manipulation\n",
    "positive_matrix = pd.DataFrame.from_dict(positive_sample_dict, orient='index').fillna(0)\n",
    "\n",
    "# Here we call our shuffle_sequences. You will be asked to modify the second parameter, word length\n",
    "negative_sequences = shuffle_sequences(positive_sequences, L = L)\n",
    "\n",
    "# Count the words in our negative_sequences {label: sequence} dictionary\n",
    "negative_sample_dict = count_words(negative_sequences, w = w)\n",
    "\n",
    "# Convert to Pandas DataFrame for easy manipulation\n",
    "negative_matrix = pd.DataFrame.from_dict(negative_sample_dict, orient='index').fillna(0)\n",
    "\n",
    "# Create the label vector, 1 = positive data, 0 = negative data\n",
    "labels = [1]*positive_matrix.shape[0] + [0]*negative_matrix.shape[0]\n",
    "\n",
    "# Combine our two genome count tables into one table, making sure to fill NAs with 0\n",
    "# NAs can occur when we append matrices with different sets of words (i.e., a word was observed in one genome)\n",
    "# but not the other)\n",
    "combined_matrix = positive_matrix.append(negative_matrix).fillna(0)\n",
    "\n",
    "# Call the classifier on our matrix and genome labels\n",
    "SVM(combined_matrix, labels, C = C, gamma = gamma, cv = cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Given the algorithm that was used to create the negative data set, does it make sense to set w = 1? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this tutorial, choose a new integer value for `w` that is greater than 1. Remember that larger values will generate more features, which requires more RAM and time, so do not set this too high.\n",
    "\n",
    "We can change the alphabet to a smaller one by setting `my_alphabet` to `PAM6_alphabet` or `three_alphabet`.\n",
    "\n",
    "**Q2: What are the potential advantages and disadvantages of recoding an alphabet of size k down to a\n",
    "smaller size (i.e., fewer characters)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the algorithm with the standard alphabet, and then the reduced alphabets. You will use the 6-category and 3-category one that I have provided, and then make up one of your own. Change the `my_alphabet` parameter to either `PAM6_alphabet` or `three_alphabet`. Create your own alphabet based on some properties or groupings of amino acids, or just make a random one if you're not so biologically inclined (though try something more interesting than I did with `awful_alphabet`, but use that as a formatting guide if you need it).\n",
    "\n",
    "**Q3: Before running the classifier, make note of the L and w parameters and be sure not to change them when comparing the effect of the alphabet size. Rerun the classifier and record your new accuracy scores with each of the different alphabets. Describe your custom alphabet and show the CV accuracy below. \"Standard\" means the default 20 amino acid alphabet with no reduction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q3_\n",
    "\n",
    "| Alphabet |  CV Accuracy  | \n",
    "| :------- | ------------- |\n",
    "| Standard |               |\n",
    "|   PAM6   |               |\n",
    "|   Three  |               |\n",
    "|  Custom  |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the SVM function we defined, you will notice that we use the option `kernel='rbf'`. This means that we are training the Support Vector Machine classifier using a radial basis function kernel.\n",
    "\n",
    "**Q4: What is the purpose of using a radial basis function kernel for SVM classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q4_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen 5 as the number of folds for cross-validation above. As you increase the number of folds, eventually you will hit the special case known as leave-one-out cross validation once the number of folds hits the number of features. In this case, only one data point is withheld, with the classifier being trained on the rest. scikit-learn has a LeaveOneOut() function that can be used for this purpose (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html).\n",
    "\n",
    "**Q5: You don’t need to run this (it will take a while!) but would you expect 5-fold or leave-one-out cross validation\n",
    "to give higher accuracy scores? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q5_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have tried the default SVM parameters, but it turns out that two parameters can exert a large influence on the final accuracy of the SVM: these are C and γ (gamma). C is the error penalty; higher values of C will induce heavier penalties on misclassified points, but can lead to overfitting (and consequently bad generalization). Gamma modifies the shape of the kernel function while retaining the same degree.\n",
    "\n",
    "One useful option offered by scikit-learn is the ability to do a grid search, which explores a range of combinations of C and γ settings. See http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html for details on the grid search implementation, if interested. We will be doing a quicker and simpler scan of the SVM parameter values.\n",
    "\n",
    "**Q6: Try searching for an optimal value of C by running the SVM classifier with at least ten different values for C. The authors of the original LIBSVM tool recommend trying `C = 2k`, for a wide range of k values. Record the relationship between k (i.e., C/2) and your cross-validation accuracy. Fill in the table below, with the first accuracy value resulting from C=1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q6_\n",
    "\n",
    "|    k     |  CV Accuracy  | \n",
    "| :------- | ------------- |\n",
    "|   0.5    |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |\n",
    "|          |               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7 [2 points]: In this tutorial we have focused on converting sequences into feature vectors, possibly with\n",
    "recoding; the SVM then takes the dot product of these vectors during training. We have used the radial basis function kernel, but any function that computes the similarity between a pair of sequences could be used. Can you suggest a different kernel function that might be useful in distinguishing alpha-helix transmembrane proteins from their randomized equivalents? You do not have to implement this function, but provide a description of how the similarity could be measured, and a brief explanation of why this might be an improvement over the standard kernels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q7_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you should have a pretty good sense of which strategies worked well and which worked less well. You might\n",
    "also want to try changing the kernel, since RBF is not always the best choice! Other options are 'linear', 'poly', and 'sigmoid'. See http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for complete details.\n",
    "\n",
    "**Q8 [2 points]: Play around with any of the parameters (but focus on C, gamma, L, w). Accuracy scores of 90% are easy to attain. Can you do better? Describe the parameters that you varied and the ranges of values that you tried. Report your highest score and each of the parameters involved in getting that score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response for Q8_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
